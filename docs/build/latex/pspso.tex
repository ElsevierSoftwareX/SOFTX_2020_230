%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{pspso}
\date{Mar 17, 2020}
\release{0.0.5}
\author{Ali Haidar}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Overview and Installation}
\label{\detokenize{index:overview-and-installation}}

\section{Overview}
\label{\detokenize{index:overview}}
pspso is a python library for selecting machine learning algorithms
parameters. The first version supports two single algorithms:
Multi\sphinxhyphen{}Layer Perceptron (MLP) and Support Vector Machine (SVM). It
supports two ensembles: Extreme Gradient Boosting (XGBoost) and Gradient
Boosting Decision Trees (GBDT).

Two types of machine learning tasks are supported by pspso:
\begin{itemize}
\item {} 
Regression.

\item {} 
Binary classification.

\end{itemize}

Three scores can be used with the first version of pspso:
\begin{itemize}
\item {} 
\sphinxstylestrong{Regression} :
\begin{itemize}
\item {} 
Root Mean Square Error (RMSE) for regression tasks

\end{itemize}

\item {} 
\sphinxstylestrong{Binary Classication} :
\begin{itemize}
\item {} 
Area under the Curve (AUC) of the Receiver Operating Characteristic (ROC)

\item {} 
Accuracy

\end{itemize}

\end{itemize}


\section{Installation}
\label{\detokenize{index:installation}}
Use the package manager \sphinxhref{https://pip.pypa.io/en/stable/}{pip} to
install pspso.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip install pspso
\end{sphinxVerbatim}


\chapter{Usage}
\label{\detokenize{index:usage}}

\section{MLP Example}
\label{\detokenize{index:mlp-example}}
pspso is used to select the machine learning algorithms parameters. It
is assumed that the user has already processed and prepared the training
and validation datasets. Below is an example for using the pso to select
the parameters of the MLP. It should be noticed that pspso handles the
MLP random weights intialization issue that may cause losing the best
solution in consecutive iterations.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pspso} \PYG{k+kn}{import} \PYG{n}{pspso}
\PYG{n}{params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{optimizer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{adam}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nadam}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sgd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{adadelta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{learning\PYGZus{}rate}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}  \PYG{p}{[}\PYG{l+m+mf}{0.01}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hiddenactivation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sigmoid}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tanh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{activation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sigmoid}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tanh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary classification}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{score}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{number\PYGZus{}of\PYGZus{}particles}\PYG{o}{=}\PYG{l+m+mi}{4}
\PYG{n}{number\PYGZus{}of\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{5}
\PYG{n}{p}\PYG{o}{=}\PYG{n}{pspso}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mlp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{params}\PYG{p}{,}\PYG{n}{task}\PYG{p}{,}\PYG{n}{score}\PYG{p}{)}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{fitpspso}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,}\PYG{n}{Y\PYGZus{}train}\PYG{p}{,}\PYG{n}{X\PYGZus{}val}\PYG{p}{,}\PYG{n}{Y\PYGZus{}val}\PYG{p}{,}\PYG{n}{number\PYGZus{}of\PYGZus{}particles}\PYG{o}{=}\PYG{n}{number\PYGZus{}of\PYGZus{}particles}\PYG{p}{,}
               \PYG{n}{number\PYGZus{}of\PYGZus{}iterations}\PYG{o}{=}\PYG{n}{number\PYGZus{}of\PYGZus{}iterations}\PYG{p}{)}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{printresults}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

In this example, four parameters were examined: optimizer,
learning\_rate, hiddenactivation, and activation. The number of neurons
in the hidden layer was kept as default.


\section{XGBoost Example}
\label{\detokenize{index:xgboost-example}}
Five parameters of the xgboost are searched and explored.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pspso} \PYG{k+kn}{import} \PYG{n}{pspso}
\PYG{n}{params} \PYG{o}{=} \PYG{p}{\PYGZob{}}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{objective}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg:tweedie}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reg:linear}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reg:gamma}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{learning\PYGZus{}rate}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}  \PYG{p}{[}\PYG{l+m+mf}{0.01}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}depth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n\PYGZus{}estimators}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{200}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{subsample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.7}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{regression}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{score}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{rmse}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{number\PYGZus{}of\PYGZus{}particles}\PYG{o}{=}\PYG{l+m+mi}{20}
\PYG{n}{number\PYGZus{}of\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{40}
\PYG{n}{p}\PYG{o}{=}\PYG{n}{pspso}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{xgboost}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{params}\PYG{p}{,}\PYG{n}{task}\PYG{p}{,}\PYG{n}{score}\PYG{p}{)}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{fitpspso}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,}\PYG{n}{Y\PYGZus{}train}\PYG{p}{,}\PYG{n}{X\PYGZus{}val}\PYG{p}{,}\PYG{n}{Y\PYGZus{}val}\PYG{p}{,}
                           \PYG{n}{number\PYGZus{}of\PYGZus{}particles}\PYG{o}{=}\PYG{n}{number\PYGZus{}of\PYGZus{}particles}\PYG{p}{,}
                           \PYG{n}{number\PYGZus{}of\PYGZus{}iterations}\PYG{o}{=}\PYG{n}{number\PYGZus{}of\PYGZus{}iterations}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PSO search:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{printresults}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\section{User Input}
\label{\detokenize{index:user-input}}
Pspso allows the user to provide a range of parameters for exploration.
The parameters vary between each algorithm.
For this current version, up to 5 paramaters can be explored at the same time.
The user can provide an empty set of parameters. By that, a default search space is created.

\sphinxstylestrong{How are the parameters encoded ?}

The parameters are encoded in json object that consists of \sphinxstyleemphasis{key,value} pairs:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{params} \PYG{o}{=} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{objective}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg:tweedie}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reg:linear}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reg:gamma}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{learning\PYGZus{}rate}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}  \PYG{p}{[}\PYG{l+m+mf}{0.01}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}depth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n\PYGZus{}estimators}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{200}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{subsample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.7}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

The key can be any parameter belonging to to the algorithm under investigation.
The value is a list.
Pspso will check the type of the first element in the list, which will determine if the values of the parameter are categorical or numerical.

\sphinxstylestrong{Categorical Parameters}

If the parameter values are \sphinxstyleemphasis{categorical}, string values are expected to be found in the list, as shown in \sphinxstyleemphasis{objective} parameter.
The values in the list will be automatically mapped into a list of integers, where each integer represents a value in the original list.
The order of the values inside the list affect the position of the value in the search space.

\sphinxstylestrong{Numerical Parameters}

On the other side, if the parameter is numerical, a list with three elements is expected {[}lb,ub, rv{]}:
\begin{itemize}
\item {} 
\sphinxstylestrong{lb}: repesents the lowest value in the search space

\item {} 
\sphinxstylestrong{ub}: represents the maximum value in the search space

\item {} 
\sphinxstylestrong{rv}: represents the number of decimal points the parameter values are rounded to before being added for training the algorithm

\end{itemize}

For e.g if you want pspso to select n\_estimators, you add the following list {[}2,200,0{]} as in the example.
By that, the lowest n\_estimators will be 2, the highest to be examined is 200, and each possible value is rounded to an integer value ( 0 decimal points).


\section{Training details}
\label{\detokenize{index:training-details}}
The user is given the chance to handle some of the default parameters
such as the number of epochs. The user can modify this by changing a
pspso class intance. For e.g., if you need to change the number of
epochs from 50 to 10 for an MLP training:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pspso} \PYG{k+kn}{import} \PYG{n}{pspso}
\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary classification}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{score}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{p}\PYG{o}{=}\PYG{n}{pspso}\PYG{o}{.}\PYG{n}{pspso}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mlp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n+nb+bp}{None}\PYG{p}{,}\PYG{n}{task}\PYG{p}{,}\PYG{n}{score}\PYG{p}{)}\PYG{c+c1}{\PYGZsh{} in case of empty set of params (None) default search space is loaded}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{defaultparams}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epochs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=}\PYG{l+m+mi}{10}
\end{sphinxVerbatim}

The verbosity can be modified for any algorithm, which allows showing details of the training process:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pspso} \PYG{k+kn}{import} \PYG{n}{pspso}
\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary classification}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{score}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{p}\PYG{o}{=}\PYG{n}{pspso}\PYG{o}{.}\PYG{n}{pspso}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mlp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n+nb+bp}{None}\PYG{p}{,}\PYG{n}{task}\PYG{p}{,}\PYG{n}{score}\PYG{p}{)}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{verbosity}\PYG{o}{=}\PYG{l+m+mi}{1}
\end{sphinxVerbatim}

Early stopping rounds for supporting algorithm can be modified, default is 60:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pspso} \PYG{k+kn}{import} \PYG{n}{pspso}
\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary classification}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{score}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{p}\PYG{o}{=}\PYG{n}{pspso}\PYG{o}{.}\PYG{n}{pspso}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{xgboost}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n+nb+bp}{None}\PYG{p}{,}\PYG{n}{task}\PYG{p}{,}\PYG{n}{score}\PYG{p}{)}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{early\PYGZus{}stopping}\PYG{o}{=}\PYG{l+m+mi}{10}
\end{sphinxVerbatim}


\chapter{Functions}
\label{\detokenize{index:functions}}

\section{ML Algorithms Functions}
\label{\detokenize{index:ml-algorithms-functions}}

\begin{savenotes}\sphinxatlongtablestart\begin{longtable}[c]{\X{1}{2}\X{1}{2}}
\hline

\endfirsthead

\multicolumn{2}{c}%
{\makebox[0pt]{\sphinxtablecontinued{\tablename\ \thetable{} \textendash{} continued from previous page}}}\\
\hline

\endhead

\hline
\multicolumn{2}{r}{\makebox[0pt][r]{\sphinxtablecontinued{Continued on next page}}}\\
\endfoot

\endlastfoot

{\hyperref[\detokenize{index:pspso.pspso.forward_prop_gbdt}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{forward\_prop\_gbdt}}}}}(particle, task, score, …)
&
Calculates the fitness value of the encoded parameters in variable particle.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.forward_prop_xgboost}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{forward\_prop\_xgboost}}}}}(particle, task, score, …)
&
This function accepts the particle from the PSO fitness function.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.forward_prop_svm}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{forward\_prop\_svm}}}}}(particle, task, score, …)
&
Train the SVM after decoding the parameters in variable particle.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.forward_prop_mlp}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{forward\_prop\_mlp}}}}}(particle, task, score, …)
&
Train the MLP after the decoding the parameters in variable particle.
\\
\hline
\end{longtable}\sphinxatlongtableend\end{savenotes}


\section{Selection Functions}
\label{\detokenize{index:selection-functions}}

\begin{savenotes}\sphinxatlongtablestart\begin{longtable}[c]{\X{1}{2}\X{1}{2}}
\hline

\endfirsthead

\multicolumn{2}{c}%
{\makebox[0pt]{\sphinxtablecontinued{\tablename\ \thetable{} \textendash{} continued from previous page}}}\\
\hline

\endhead

\hline
\multicolumn{2}{r}{\makebox[0pt][r]{\sphinxtablecontinued{Continued on next page}}}\\
\endfoot

\endlastfoot

{\hyperref[\detokenize{index:pspso.pspso.fitpspso}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{fitpspso}}}}}({[}X\_train, Y\_train, X\_val, Y\_val, …{]})
&
fitpso search
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.fitpsgrid}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{fitpsgrid}}}}}({[}X\_train, Y\_train, X\_val, Y\_val{]})
&
Grid search was implemented to match the training process with pspso and for comparison purposes.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.fitpsrandom}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{fitpsrandom}}}}}({[}X\_train, Y\_train, X\_val, …{]})
&
With Random search, the process is done for number of times specified by a parameter in the function.
\\
\hline
\end{longtable}\sphinxatlongtableend\end{savenotes}


\chapter{Summary}
\label{\detokenize{index:summary}}\index{pspso (class in pspso)@\spxentry{pspso}\spxextra{class in pspso}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{pspso.}}\sphinxbfcode{\sphinxupquote{pspso}}}{\emph{estimator=\textquotesingle{}xgboost\textquotesingle{}}, \emph{params=None}, \emph{task=\textquotesingle{}regression\textquotesingle{}}, \emph{score=\textquotesingle{}rmse\textquotesingle{}}}{}
This class searches for algorithm parameters by using the Particle Swarm Optimization (PSO) algorithm.
\index{calculatecombinations() (pspso.pspso method)@\spxentry{calculatecombinations()}\spxextra{pspso.pspso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.calculatecombinations}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{calculatecombinations}}}{}{}
generate combinations

\end{fulllineitems}

\index{decode\_parameters() (pspso.pspso static method)@\spxentry{decode\_parameters()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.decode_parameters}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{decode\_parameters}}}{\emph{particle}}{}
Decodes the parameters of a list into a meaningful set of parameters.
To decode a particle, we need the following global variables:
\begin{quote}

global variable parameters
global variable defaultparameters
global variable paramdetails
global variable rounding
\end{quote}

\end{fulllineitems}

\index{f() (pspso.pspso static method)@\spxentry{f()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.f}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{f}}}{\emph{q}, \emph{estimator}, \emph{task}, \emph{score}, \emph{X\_train}, \emph{Y\_train}, \emph{X\_val}, \emph{Y\_val}}{}
Higher\sphinxhyphen{}level method to do forward\_prop in the
whole swarm.

Inputs
\begin{description}
\item[{x: numpy.ndarray of shape (n\_particles, dimensions)}] \leavevmode
The swarm that will perform the search

\end{description}

Returns
\begin{description}
\item[{numpy.ndarray of shape (n\_particles, )}] \leavevmode
The computed loss for each particle

\end{description}

\end{fulllineitems}

\index{fitpsgrid() (pspso.pspso method)@\spxentry{fitpsgrid()}\spxextra{pspso.pspso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.fitpsgrid}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fitpsgrid}}}{\emph{X\_train=None}, \emph{Y\_train=None}, \emph{X\_val=None}, \emph{Y\_val=None}}{}
Grid search was implemented to match the training process with pspso and for comparison purposes.
I have to traverse each value between x\_min, x\_max. Create a list seperating rounding value.

\end{fulllineitems}

\index{fitpspso() (pspso.pspso method)@\spxentry{fitpspso()}\spxextra{pspso.pspso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.fitpspso}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fitpspso}}}{\emph{X\_train=None}, \emph{Y\_train=None}, \emph{X\_val=None}, \emph{Y\_val=None}, \emph{number\_of\_particles=2}, \emph{number\_of\_iterations=2}, \emph{options=\{\textquotesingle{}c1\textquotesingle{}: 0.5}, \emph{\textquotesingle{}c2\textquotesingle{}: 0.3}, \emph{\textquotesingle{}w\textquotesingle{}: 0.4\}}}{}
fitpso search

\end{fulllineitems}

\index{fitpsrandom() (pspso.pspso method)@\spxentry{fitpsrandom()}\spxextra{pspso.pspso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.fitpsrandom}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fitpsrandom}}}{\emph{X\_train=None}, \emph{Y\_train=None}, \emph{X\_val=None}, \emph{Y\_val=None}, \emph{number\_of\_attempts=20}}{}
With Random search, the process is done for number of times specified by a parameter in the function.

\end{fulllineitems}

\index{forward\_prop\_gbdt() (pspso.pspso static method)@\spxentry{forward\_prop\_gbdt()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.forward_prop_gbdt}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{forward\_prop\_gbdt}}}{\emph{particle}, \emph{task}, \emph{score}, \emph{X\_train}, \emph{Y\_train}, \emph{X\_val}, \emph{Y\_val}}{}
Calculates the fitness value of the encoded parameters in variable particle.
The particle is decoded into parameters of the gbdt. Then, The gbdt is trained and the score is sent back to the fitness function.

Inputs
\begin{description}
\item[{particle: list of values (n dimensions)}] \leavevmode
A particle in the swarm

\item[{task: regression, binary classification, or binary classification r}] \leavevmode
the task to be conducted

\item[{score: rmse (regression), auc (binary classification), acc (binary classification)}] \leavevmode
the type of evaluation

\item[{X\_train: numpy.ndarray of shape (m, n)}] \leavevmode
Training dataset

\item[{Y\_train: numpy.ndarray of shape (m,1)}] \leavevmode
Training target

\item[{X\_val: numpy.ndarray of shape (x, y)}] \leavevmode
Validation dataset

\item[{Y\_val: numpy.ndarray of shape (x,1)}] \leavevmode
Validation target

\end{description}

Returns
\begin{description}
\item[{variable, model}] \leavevmode
the score of the trained algorithm over the validation dataset, trained model

\end{description}

\end{fulllineitems}

\index{forward\_prop\_mlp() (pspso.pspso static method)@\spxentry{forward\_prop\_mlp()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.forward_prop_mlp}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{forward\_prop\_mlp}}}{\emph{particle}, \emph{task}, \emph{score}, \emph{X\_train}, \emph{Y\_train}, \emph{X\_val}, \emph{Y\_val}}{}
Train the MLP after the decoding the parameters in variable particle.

\end{fulllineitems}

\index{forward\_prop\_svm() (pspso.pspso static method)@\spxentry{forward\_prop\_svm()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.forward_prop_svm}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{forward\_prop\_svm}}}{\emph{particle}, \emph{task}, \emph{score}, \emph{X\_train}, \emph{Y\_train}, \emph{X\_val}, \emph{Y\_val}}{}
Train the SVM after decoding the parameters in variable particle.

\end{fulllineitems}

\index{forward\_prop\_xgboost() (pspso.pspso static method)@\spxentry{forward\_prop\_xgboost()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.forward_prop_xgboost}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{forward\_prop\_xgboost}}}{\emph{particle}, \emph{task}, \emph{score}, \emph{X\_train}, \emph{Y\_train}, \emph{X\_val}, \emph{Y\_val}}{}
This function accepts the particle from the PSO fitness function.
The particle is decoded into parameters of the XGBoost.
This function is similar to forward\_prop\_gbdt
The gbdt is trained and the score is sent back to the fitness function.

Inputs
\begin{description}
\item[{particle: list of values (n dimensions)}] \leavevmode
A particle in the swarm

\item[{task: regression, binary classification, or binary classification r}] \leavevmode
the task to be conducted

\item[{score: rmse (regression), auc (binary classification), acc (binary classification)}] \leavevmode
the type of evaluation

\item[{X\_train: numpy.ndarray of shape (m, n)}] \leavevmode
Training dataset

\item[{Y\_train: numpy.ndarray of shape (m,1)}] \leavevmode
Training target

\item[{X\_val: numpy.ndarray of shape (x, y)}] \leavevmode
Validation dataset

\item[{Y\_val: numpy.ndarray of shape (x,1)}] \leavevmode
Validation target

\end{description}

Returns
\begin{description}
\item[{variable, model}] \leavevmode
the score of the trained algorithm over the validation dataset, trained model

\end{description}

\end{fulllineitems}

\index{printresults() (pspso.pspso method)@\spxentry{printresults()}\spxextra{pspso.pspso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.printresults}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{printresults}}}{}{}
print results

\end{fulllineitems}

\index{readparameters() (pspso.pspso static method)@\spxentry{readparameters()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.readparameters}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{readparameters}}}{\emph{params=None}, \emph{estimator=None}, \emph{task=None}}{}
read the parameters provided by the user.

\end{fulllineitems}

\index{rebuildmodel() (pspso.pspso static method)@\spxentry{rebuildmodel()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.rebuildmodel}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{rebuildmodel}}}{\emph{estimator}, \emph{pos}, \emph{task}, \emph{score}, \emph{X\_train}, \emph{Y\_train}, \emph{X\_val}, \emph{Y\_val}}{}
Used to rebuild the model after selecting the parameters.

\end{fulllineitems}


\end{fulllineitems}



\chapter{Future Work}
\label{\detokenize{index:future-work}}

\section{Additional Parameters}
\label{\detokenize{index:additional-parameters}}
To add new parameters to the currently supported algorithms, two functions should be updated

The \sphinxstylestrong{read\_params} function should include default details about the parameter,
The \sphinxstylestrong{forward\_prop\_algorithmname} function should add the parameter to the initialization process


\section{New Algorithms}
\label{\detokenize{index:new-algorithms}}
Adding a new algorithm is more complex as you will be required to add an objective function that will detail the training and evaluation process.

New Optimizers
Two main optimizers are currently supported. These algorithms are built in the pyswams function.

The default is globalbest pso, however the user can specify the local pso
The pso parameters are set to default in each case and can be modified by the user.


\section{Crossvalidation}
\label{\detokenize{index:crossvalidation}}
We are working towards adding the cross validation support that will take the training data and number of folds, then split the records and train each fold. Finally, the average performance is retuned to the user.


\section{Multi\sphinxhyphen{}Class Classification}
\label{\detokenize{index:multi-class-classification}}
We are also working on adding multi\sphinxhyphen{}class classification and data oversampling techniques.


\chapter{Contributing}
\label{\detokenize{index:contributing}}
Pull requests are welcome. For major changes, please open an issue first
to discuss what you would like to change.

Please make sure to update tests as appropriate.

We are working towards adding the cross validation support that will take the training data and number of folds, then split the records and train each fold. Finally, the average performance is retuned to the user.

We are also working on adding multi\sphinxhyphen{}class classification and data oversampling techniques.


\chapter{License}
\label{\detokenize{index:license}}
Copyright (c) {[}2020{]} {[}Ali Haidar{]}

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the “Software”), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}