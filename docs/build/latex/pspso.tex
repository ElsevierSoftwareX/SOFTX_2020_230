%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\usepackage{sphinxmessages}




\title{pspso}
\date{Apr 10, 2020}
\release{0.0.9}
\author{Ali Haidar}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}
\noindent{\hspace*{\fill}\sphinxincludegraphics{{LOGO}.png}\hspace*{\fill}}




\chapter{Overview and Installation}
\label{\detokenize{index:overview-and-installation}}

\section{Overview}
\label{\detokenize{index:overview}}
\sphinxstylestrong{pspso} is a python library for selecting machine learning algorithms
parameters. The first version supports two single algorithms:
Multi\sphinxhyphen{}Layer Perceptron (MLP) and Support Vector Machine (SVM). It
supports two ensembles: Extreme Gradient Boosting (XGBoost) and Gradient
Boosting Decision Trees (GBDT).

Two types of machine learning tasks are supported by pspso:
\begin{itemize}
\item {} 
Regression.

\item {} 
Binary classification.

\end{itemize}

Three scores are supported in the first version of pspso:
\begin{itemize}
\item {} 
\sphinxstylestrong{Regression} :
\begin{itemize}
\item {} 
Root Mean Square Error (RMSE)

\end{itemize}

\item {} 
\sphinxstylestrong{Binary Classication} :
\begin{itemize}
\item {} 
Area under the Curve (AUC) of the Receiver Operating Characteristic (ROC)

\item {} 
Accuracy

\end{itemize}

\end{itemize}


\section{Installation}
\label{\detokenize{index:installation}}
Use the package manager \sphinxhref{https://pip.pypa.io/en/stable/}{pip} to
install pspso.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip install pspso
\end{sphinxVerbatim}


\chapter{Usage}
\label{\detokenize{index:usage}}

\section{MLP Example (Binary Classification)}
\label{\detokenize{index:mlp-example-binary-classification}}
\sphinxstylestrong{pspso} is used to select the machine learning algorithms parameters.
Below is an example for using the pspso to select
the parameters of the MLP. pspso handles the
MLP random weights intialization issue that may cause losing the best
solution in consecutive iterations.

The following example demonstrates the selection process of the MLP parameters.
A variable named \sphinxstyleemphasis{params} was not given by the user. Hence, the default search space of the MLP is loaded.
This search space contains five parameters:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{optimizer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{RMSprop}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{adam}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sgd}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{adamax}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nadam}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{adadelta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{p}{,}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{learning\PYGZus{}rate}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}  \PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{l+m+mf}{0.3}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{neurons}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{40}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hiddenactivation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sigmoid}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tanh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{activation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sigmoid}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tanh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

The task and the score were defined as \sphinxstyleemphasis{binary classification} and \sphinxstyleemphasis{auc} respectively.
Then, the PSO was used to select the parameters of the MLP.
Results are provided back to the user through the \sphinxstylestrong{print\_results()} function.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn.preprocessing} \PYG{k+kn}{import} \PYG{n}{MinMaxScaler}
\PYG{k+kn}{from} \PYG{n+nn}{pspso} \PYG{k+kn}{import} \PYG{n}{pspso}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn} \PYG{k+kn}{import} \PYG{n}{datasets}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn.model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{train\PYGZus{}test\PYGZus{}split}

\PYG{n}{breastcancer} \PYG{o}{=} \PYG{n}{datasets}\PYG{o}{.}\PYG{n}{load\PYGZus{}breast\PYGZus{}cancer}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{=}\PYG{n}{breastcancer}\PYG{o}{.}\PYG{n}{data}\PYG{c+c1}{\PYGZsh{}get the breast cancer dataset input features}
\PYG{n}{target}\PYG{o}{=}\PYG{n}{breastcancer}\PYG{o}{.}\PYG{n}{target}\PYG{c+c1}{\PYGZsh{} target}
\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{Y\PYGZus{}train}\PYG{p}{,} \PYG{n}{Y\PYGZus{}test} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{target}\PYG{p}{,}\PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{,}\PYG{n}{stratify}\PYG{o}{=}\PYG{n}{target}\PYG{p}{)}
\PYG{n}{normalize} \PYG{o}{=} \PYG{n}{MinMaxScaler}\PYG{p}{(}\PYG{n}{feature\PYGZus{}range}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{c+c1}{\PYGZsh{}normalize input features}
\PYG{n}{X\PYGZus{}train}\PYG{o}{=}\PYG{n}{normalize}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{)}
\PYG{n}{X\PYGZus{}test}\PYG{o}{=}\PYG{n}{normalize}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}val}\PYG{p}{,} \PYG{n}{Y\PYGZus{}train}\PYG{p}{,} \PYG{n}{Y\PYGZus{}val} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{Y\PYGZus{}train}\PYG{p}{,}\PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.15}\PYG{p}{,}\PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{,}\PYG{n}{stratify}\PYG{o}{=}\PYG{n}{Y\PYGZus{}train}\PYG{p}{)}
\PYG{n}{p}\PYG{o}{=}\PYG{n}{pspso}\PYG{p}{(}\PYG{n}{estimator}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mlp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary classification}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{score}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{pos}\PYG{p}{,}\PYG{n}{cost}\PYG{p}{,}\PYG{n}{duration}\PYG{p}{,}\PYG{n}{model}\PYG{p}{,}\PYG{n}{optimizer}\PYG{o}{=}\PYG{n}{p}\PYG{o}{.}\PYG{n}{fitpspso}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,}\PYG{n}{Y\PYGZus{}train}\PYG{p}{,}\PYG{n}{X\PYGZus{}val}\PYG{p}{,}\PYG{n}{Y\PYGZus{}val}\PYG{p}{)}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{print\PYGZus{}results}\PYG{p}{(}\PYG{p}{)}\PYG{c+c1}{\PYGZsh{}print the results}
\PYG{n}{testscore}\PYG{o}{=}\PYG{n}{pspso}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{p}\PYG{o}{.}\PYG{n}{model}\PYG{p}{,}\PYG{n}{p}\PYG{o}{.}\PYG{n}{estimator}\PYG{p}{,}\PYG{n}{p}\PYG{o}{.}\PYG{n}{task}\PYG{p}{,}\PYG{n}{p}\PYG{o}{.}\PYG{n}{score}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{Y\PYGZus{}test}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{testscore}\PYG{p}{)}
\end{sphinxVerbatim}

In this example, four parameters were examined: optimizer,
learning\_rate, hiddenactivation, and activation. The number of neurons
in the hidden layer was kept as default.

Output:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Estimator}\PYG{p}{:} \PYG{n}{mlp}
\PYG{n}{Task}\PYG{p}{:} \PYG{n}{binary} \PYG{n}{classification}
\PYG{n}{Selection} \PYG{n+nb}{type}\PYG{p}{:} \PYG{n}{PSO}
\PYG{n}{Number} \PYG{n}{of} \PYG{n}{attempts}\PYG{p}{:}\PYG{l+m+mi}{50}
\PYG{n}{Total} \PYG{n}{number} \PYG{n}{of} \PYG{n}{combinations}\PYG{p}{:} \PYG{l+m+mi}{45360}
\PYG{n}{Parameters}\PYG{p}{:}
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nadam}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{learning\PYGZus{}rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.29}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{neurons}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hiddenactivation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sigmoid}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{activation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sigmoid}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}
\PYG{n}{Global} \PYG{n}{best} \PYG{n}{position}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{3.8997699}  \PYG{l+m+mf}{0.28725911} \PYG{l+m+mf}{4.21218138} \PYG{l+m+mf}{1.41200923} \PYG{l+m+mf}{0.84643591}\PYG{p}{]}
\PYG{n}{Global} \PYG{n}{best} \PYG{n}{cost}\PYG{p}{:} \PYG{l+m+mf}{0.0}
\PYG{n}{Time} \PYG{n}{taken} \PYG{n}{to} \PYG{n}{find} \PYG{n}{the} \PYG{n+nb}{set} \PYG{n}{of} \PYG{n}{parameters}\PYG{p}{:} \PYG{l+m+mf}{160.3374378681183}
\PYG{n}{Number} \PYG{n}{of} \PYG{n}{particles}\PYG{p}{:} \PYG{l+m+mi}{5}
\PYG{n}{Number} \PYG{n}{of} \PYG{n}{iterations}\PYG{p}{:} \PYG{l+m+mi}{10}
\PYG{l+m+mf}{0.9867724867724867}
\end{sphinxVerbatim}


\section{XGBoost Example (Binary Classification)}
\label{\detokenize{index:xgboost-example-binary-classification}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn.preprocessing} \PYG{k+kn}{import} \PYG{n}{MinMaxScaler}
\PYG{k+kn}{from} \PYG{n+nn}{pspso} \PYG{k+kn}{import} \PYG{n}{pspso}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn} \PYG{k+kn}{import} \PYG{n}{datasets}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn.model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{train\PYGZus{}test\PYGZus{}split}

\PYG{n}{breastcancer} \PYG{o}{=} \PYG{n}{datasets}\PYG{o}{.}\PYG{n}{load\PYGZus{}breast\PYGZus{}cancer}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{=}\PYG{n}{breastcancer}\PYG{o}{.}\PYG{n}{data}\PYG{c+c1}{\PYGZsh{}get the breast cancer dataset input features}
\PYG{n}{target}\PYG{o}{=}\PYG{n}{breastcancer}\PYG{o}{.}\PYG{n}{target}\PYG{c+c1}{\PYGZsh{} target}
\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{Y\PYGZus{}train}\PYG{p}{,} \PYG{n}{Y\PYGZus{}test} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{target}\PYG{p}{,}\PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{,}\PYG{n}{stratify}\PYG{o}{=}\PYG{n}{target}\PYG{p}{)}
\PYG{n}{normalize} \PYG{o}{=} \PYG{n}{MinMaxScaler}\PYG{p}{(}\PYG{n}{feature\PYGZus{}range}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{c+c1}{\PYGZsh{}normalize input features}
\PYG{n}{X\PYGZus{}train}\PYG{o}{=}\PYG{n}{normalize}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{)}
\PYG{n}{X\PYGZus{}test}\PYG{o}{=}\PYG{n}{normalize}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}val}\PYG{p}{,} \PYG{n}{Y\PYGZus{}train}\PYG{p}{,} \PYG{n}{Y\PYGZus{}val} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{Y\PYGZus{}train}\PYG{p}{,}\PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.15}\PYG{p}{,}\PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{,}\PYG{n}{stratify}\PYG{o}{=}\PYG{n}{Y\PYGZus{}train}\PYG{p}{)}

\PYG{n}{params} \PYG{o}{=} \PYG{p}{\PYGZob{}}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{learning\PYGZus{}rate}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}  \PYG{p}{[}\PYG{l+m+mf}{0.01}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}depth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n\PYGZus{}estimators}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{200}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{subsample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.7}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\PYG{n}{p}\PYG{o}{=}\PYG{n}{pspso}\PYG{p}{(}\PYG{n}{estimator}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{xgboost}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{params}\PYG{o}{=}\PYG{n}{params}\PYG{p}{,}\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary classification}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{score}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{pos}\PYG{p}{,}\PYG{n}{cost}\PYG{p}{,}\PYG{n}{duration}\PYG{p}{,}\PYG{n}{model}\PYG{p}{,}\PYG{n}{optimizer}\PYG{o}{=}\PYG{n}{p}\PYG{o}{.}\PYG{n}{fitpspso}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,}\PYG{n}{Y\PYGZus{}train}\PYG{p}{,}\PYG{n}{X\PYGZus{}val}\PYG{p}{,}\PYG{n}{Y\PYGZus{}val}\PYG{p}{)}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{print\PYGZus{}results}\PYG{p}{(}\PYG{p}{)}\PYG{c+c1}{\PYGZsh{}print the results}
\PYG{n}{testscore}\PYG{o}{=}\PYG{n}{pspso}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{p}\PYG{o}{.}\PYG{n}{model}\PYG{p}{,}\PYG{n}{p}\PYG{o}{.}\PYG{n}{estimator}\PYG{p}{,}\PYG{n}{p}\PYG{o}{.}\PYG{n}{task}\PYG{p}{,}\PYG{n}{p}\PYG{o}{.}\PYG{n}{score}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{Y\PYGZus{}test}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{testscore}\PYG{p}{)}
\end{sphinxVerbatim}


\section{XGBoost Example (Regression)}
\label{\detokenize{index:xgboost-example-regression}}
The XGBoost is an implementation of boosting decision trees.
Five parameters were utilized for selection: objective, learning rate, maximum depth, number of estimators, and subsample.
Three categorical values were selected for the objective parameter.
The learning rate parameter values range between \sphinxstyleemphasis{0.01} and \sphinxstyleemphasis{0.2} with \sphinxstyleemphasis{2} decimal point,
maximum depth ranges between \sphinxstyleemphasis{1} and \sphinxstyleemphasis{10} with \sphinxstyleemphasis{0} decimal points \sphinxstyleemphasis{(1,2,3,4,5,6,7,8,9,10)}, etc.
The task and score are selected as regression and RMSE respectively.
The number of particles and number of iterations can be left as default values if needed.
Then, a pspso instance is created. By applying the fitpspso function, the selection process is applied.
Finally, results are printed back to the user.
The best model, best parameters, score, time, and other details will be saved in the created instance for the user to check.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn.preprocessing} \PYG{k+kn}{import} \PYG{n}{MinMaxScaler}
\PYG{k+kn}{from} \PYG{n+nn}{pspso} \PYG{k+kn}{import} \PYG{n}{pspso}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn} \PYG{k+kn}{import} \PYG{n}{datasets}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn.model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{train\PYGZus{}test\PYGZus{}split}

\PYG{n}{boston\PYGZus{}data} \PYG{o}{=} \PYG{n}{datasets}\PYG{o}{.}\PYG{n}{load\PYGZus{}boston}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{=}\PYG{n}{boston\PYGZus{}data}\PYG{o}{.}\PYG{n}{data}
\PYG{n}{target}\PYG{o}{=}\PYG{n}{boston\PYGZus{}data}\PYG{o}{.}\PYG{n}{target}

\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{Y\PYGZus{}train}\PYG{p}{,} \PYG{n}{Y\PYGZus{}test} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{target}\PYG{p}{,}\PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{normalize} \PYG{o}{=} \PYG{n}{MinMaxScaler}\PYG{p}{(}\PYG{n}{feature\PYGZus{}range}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{c+c1}{\PYGZsh{}normalize input features}
\PYG{n}{normalizetarget} \PYG{o}{=} \PYG{n}{MinMaxScaler}\PYG{p}{(}\PYG{n}{feature\PYGZus{}range}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{c+c1}{\PYGZsh{}normalize target}

\PYG{n}{X\PYGZus{}train}\PYG{o}{=}\PYG{n}{normalize}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{)}
\PYG{n}{X\PYGZus{}test}\PYG{o}{=}\PYG{n}{normalize}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{Y\PYGZus{}train}\PYG{o}{=}\PYG{n}{normalizetarget}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{Y\PYGZus{}train}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{Y\PYGZus{}test}\PYG{o}{=}\PYG{n}{normalizetarget}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{Y\PYGZus{}test}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}val}\PYG{p}{,} \PYG{n}{Y\PYGZus{}train}\PYG{p}{,} \PYG{n}{Y\PYGZus{}val} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{Y\PYGZus{}train}\PYG{p}{,}\PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.25}\PYG{p}{,}\PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{params} \PYG{o}{=} \PYG{p}{\PYGZob{}}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{objective}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg:tweedie}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reg:linear}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reg:gamma}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{learning\PYGZus{}rate}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}  \PYG{p}{[}\PYG{l+m+mf}{0.01}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}depth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n\PYGZus{}estimators}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{200}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{subsample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.7}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\PYG{n}{p}\PYG{o}{=}\PYG{n}{pspso}\PYG{p}{(}\PYG{n}{estimator}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{xgboost}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{params}\PYG{o}{=}\PYG{n}{params}\PYG{p}{,}\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{regression}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{score}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rmse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{pos}\PYG{p}{,}\PYG{n}{cost}\PYG{p}{,}\PYG{n}{duration}\PYG{p}{,}\PYG{n}{model}\PYG{p}{,}\PYG{n}{optimizer}\PYG{o}{=}\PYG{n}{p}\PYG{o}{.}\PYG{n}{fitpspso}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,}\PYG{n}{Y\PYGZus{}train}\PYG{p}{,}\PYG{n}{X\PYGZus{}val}\PYG{p}{,}\PYG{n}{Y\PYGZus{}val}\PYG{p}{)}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{print\PYGZus{}results}\PYG{p}{(}\PYG{p}{)}\PYG{c+c1}{\PYGZsh{}print the results}
\PYG{n}{testscore}\PYG{o}{=}\PYG{n}{pspso}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{p}\PYG{o}{.}\PYG{n}{model}\PYG{p}{,}\PYG{n}{p}\PYG{o}{.}\PYG{n}{estimator}\PYG{p}{,}\PYG{n}{p}\PYG{o}{.}\PYG{n}{task}\PYG{p}{,}\PYG{n}{p}\PYG{o}{.}\PYG{n}{score}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{Y\PYGZus{}test}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{n}{testscore}\PYG{p}{)}
\end{sphinxVerbatim}


\section{User Input}
\label{\detokenize{index:user-input}}
The user is required to select the type of the algorithm (‘mlp’, ‘svm’, ‘xgboost’, ‘gbdt’); the task type (‘binary classification’,’regression’), score (‘rmse’, ‘acc’, or ‘auc’). The user can keep the parameters variable empty, where a default set of parameters and ranges is loaded
for each algorithm.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pspso} \PYG{k+kn}{import} \PYG{n}{pspso}
\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary classification}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{score}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{p}\PYG{o}{=}\PYG{n}{pspso}\PYG{o}{.}\PYG{n}{pspso}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{xgboost}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n+nb+bp}{None}\PYG{p}{,}\PYG{n}{task}\PYG{p}{,}\PYG{n}{score}\PYG{p}{)}
\end{sphinxVerbatim}

Pspso allows the user to provide a range of parameters for exploration.
The parameters vary between each algorithm. Any parameter supported by the Scikit\sphinxhyphen{}Learn API for GBDT and XGBoost can be added to the selection process.
A set of parameters that contains five XGBoost parameters is shown below. The parameters are encoded in JSON object that consists of \sphinxstyleemphasis{key,value} pairs:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{objective}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg:tweedie}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reg:linear}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reg:gamma}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{learning\PYGZus{}rate}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}  \PYG{p}{[}\PYG{l+m+mf}{0.01}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}depth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n\PYGZus{}estimators}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{200}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{subsample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.7}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

The key can be any parameter belonging to to the algorithm under investigation.
The value is a list.
Pspso will check the type of the first element in the list, which will determine if the values of the parameter are categorical or numerical.

\sphinxstylestrong{Categorical Parameters}

If the parameter values are \sphinxstyleemphasis{categorical}, string values are expected to be found in the list, as shown in \sphinxstyleemphasis{objective} parameter.
The values in the list will be automatically mapped into a list of integers, where each integer represents a value in the original list.
The order of the values inside the list affect the position of the value in the search space.

\sphinxstylestrong{Numerical Parameters}

If the parameter is numerical, a list of three elements {[}lb,ub, rv{]} is expected to be found:
\begin{itemize}
\item {} 
\sphinxstylestrong{lb}: repesents the lowest value in the search space

\item {} 
\sphinxstylestrong{ub}: represents the maximum value in the search space

\item {} 
\sphinxstylestrong{rv}: represents the number of decimal points the parameter values are rounded to before being added for training the algorithm

\end{itemize}

For e.g if you want pspso to select n\_estimators, add the following list \sphinxstyleemphasis{{[}2,200,0{]}}.
By that, the lowest n\_estimators will be \sphinxstyleemphasis{2}, the highest to be examined is \sphinxstyleemphasis{200}, and each possible value is rounded to an integer value ( \sphinxstyleemphasis{0} decimal points).

\sphinxstylestrong{Other parameters}

The user is given the chance to handle some of the default parameters
such as the number of epochs in the MLP. Although this parameter can be optimized, but its not encouraged. The user can modify this by changing a
pspso class instance. For e.g., to change the number of
epochs from default to 10 in MLP training:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pspso} \PYG{k+kn}{import} \PYG{n}{pspso}
\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary classification}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{score}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{p}\PYG{o}{=}\PYG{n}{pspso}\PYG{o}{.}\PYG{n}{pspso}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mlp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n+nb+bp}{None}\PYG{p}{,}\PYG{n}{task}\PYG{p}{,}\PYG{n}{score}\PYG{p}{)}\PYG{c+c1}{\PYGZsh{} in case of empty set of params (None) default search space is loaded}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{defaultparams}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epochs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=}\PYG{l+m+mi}{10}
\end{sphinxVerbatim}

The verbosity can be modified for any algorithm, which allows showing details of the training process:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pspso} \PYG{k+kn}{import} \PYG{n}{pspso}
\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary classification}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{score}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{p}\PYG{o}{=}\PYG{n}{pspso}\PYG{o}{.}\PYG{n}{pspso}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mlp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n+nb+bp}{None}\PYG{p}{,}\PYG{n}{task}\PYG{p}{,}\PYG{n}{score}\PYG{p}{)}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{verbosity}\PYG{o}{=}\PYG{l+m+mi}{1}
\end{sphinxVerbatim}

Early stopping rounds can alos be modified, the user can set a value different to the default value:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pspso} \PYG{k+kn}{import} \PYG{n}{pspso}
\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary classification}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{score}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{p}\PYG{o}{=}\PYG{n}{pspso}\PYG{o}{.}\PYG{n}{pspso}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{xgboost}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n+nb+bp}{None}\PYG{p}{,}\PYG{n}{task}\PYG{p}{,}\PYG{n}{score}\PYG{p}{)}
\PYG{n}{p}\PYG{o}{.}\PYG{n}{early\PYGZus{}stopping}\PYG{o}{=}\PYG{l+m+mi}{10}
\end{sphinxVerbatim}

Other parameters such that n\_jobs in XGBoost can also be modified before the start of the selection process.


\chapter{Functions}
\label{\detokenize{index:functions}}

\section{ML Algorithms Functions}
\label{\detokenize{index:ml-algorithms-functions}}

\begin{savenotes}\sphinxatlongtablestart\begin{longtable}[c]{\X{1}{2}\X{1}{2}}
\hline

\endfirsthead

\multicolumn{2}{c}%
{\makebox[0pt]{\sphinxtablecontinued{\tablename\ \thetable{} \textendash{} continued from previous page}}}\\
\hline

\endhead

\hline
\multicolumn{2}{r}{\makebox[0pt][r]{\sphinxtablecontinued{Continued on next page}}}\\
\endfoot

\endlastfoot

{\hyperref[\detokenize{index:pspso.pspso.forward_prop_gbdt}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{forward\_prop\_gbdt}}}}}(particle, task, score, …)
&
Train the GBDT after decoding the parameters in variable particle.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.forward_prop_xgboost}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{forward\_prop\_xgboost}}}}}(particle, task, score, …)
&
Train the XGBoost after decoding the parameters in variable particle.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.forward_prop_svm}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{forward\_prop\_svm}}}}}(particle, task, score, …)
&
Train the SVM after decoding the parameters in variable particle.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.forward_prop_mlp}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{forward\_prop\_mlp}}}}}(particle, task, score, …)
&
Train the MLP after the decoding the parameters in variable particle.
\\
\hline
\end{longtable}\sphinxatlongtableend\end{savenotes}


\section{Selection Functions}
\label{\detokenize{index:selection-functions}}

\begin{savenotes}\sphinxatlongtablestart\begin{longtable}[c]{\X{1}{2}\X{1}{2}}
\hline

\endfirsthead

\multicolumn{2}{c}%
{\makebox[0pt]{\sphinxtablecontinued{\tablename\ \thetable{} \textendash{} continued from previous page}}}\\
\hline

\endhead

\hline
\multicolumn{2}{r}{\makebox[0pt][r]{\sphinxtablecontinued{Continued on next page}}}\\
\endfoot

\endlastfoot

{\hyperref[\detokenize{index:pspso.pspso.fitpspso}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{fitpspso}}}}}({[}X\_train, Y\_train, X\_val, Y\_val, …{]})
&
Select the algorithm parameters based on PSO.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.fitpsgrid}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{fitpsgrid}}}}}({[}X\_train, Y\_train, X\_val, Y\_val{]})
&
Select the algorithm parameters based on Grid search.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.fitpsrandom}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{fitpsrandom}}}}}({[}X\_train, Y\_train, X\_val, …{]})
&
Select the algorithm parameters based on radnom search.
\\
\hline
\end{longtable}\sphinxatlongtableend\end{savenotes}

The fitpsrandom() and fitpsgrid() were implmented as two default selection methods.
With fit random search, the number of attempts to be tried is added by the user as a variable.
In grid search, all the possible combinations are created and investigated by the package.
These functions follow the same encoding schema used in fitpspso(), and were basically added for comparison.


\section{Parameters Functions}
\label{\detokenize{index:parameters-functions}}

\begin{savenotes}\sphinxatlongtablestart\begin{longtable}[c]{\X{1}{2}\X{1}{2}}
\hline

\endfirsthead

\multicolumn{2}{c}%
{\makebox[0pt]{\sphinxtablecontinued{\tablename\ \thetable{} \textendash{} continued from previous page}}}\\
\hline

\endhead

\hline
\multicolumn{2}{r}{\makebox[0pt][r]{\sphinxtablecontinued{Continued on next page}}}\\
\endfoot

\endlastfoot

{\hyperref[\detokenize{index:pspso.pspso.read_parameters}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{read\_parameters}}}}}({[}params, estimator, task{]})
&
Read the parameters provided by the user.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.decode_parameters}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{decode\_parameters}}}}}(particle)
&
Decodes the parameters of a list into a meaningful set of parameters.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.get_default_params}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{get\_default\_params}}}}}(estimator, task)
&
Set the default parameters of the estimator.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.get_default_search_space}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{get\_default\_search\_space}}}}}(estimator, task)
&
Create a dictionary of default parameters if the user didnt provide parameters.
\\
\hline
\end{longtable}\sphinxatlongtableend\end{savenotes}


\section{Other Functions}
\label{\detokenize{index:other-functions}}

\begin{savenotes}\sphinxatlongtablestart\begin{longtable}[c]{\X{1}{2}\X{1}{2}}
\hline

\endfirsthead

\multicolumn{2}{c}%
{\makebox[0pt]{\sphinxtablecontinued{\tablename\ \thetable{} \textendash{} continued from previous page}}}\\
\hline

\endhead

\hline
\multicolumn{2}{r}{\makebox[0pt][r]{\sphinxtablecontinued{Continued on next page}}}\\
\endfoot

\endlastfoot

{\hyperref[\detokenize{index:pspso.pspso.f}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{f}}}}}(q, estimator, task, score, X\_train, …)
&
Higher\sphinxhyphen{}level method to do forward\_prop in the whole swarm.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.rebuildmodel}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{rebuildmodel}}}}}(estimator, pos, task, score, …)
&
Used to rebuild the model after selecting the parameters.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.print_results}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{print\_results}}}}}()
&
Print the results found in the pspso instance.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.calculatecombinations}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{calculatecombinations}}}}}()
&
A function that will generate all the possible combinations in the search space.
\\
\hline
{\hyperref[\detokenize{index:pspso.pspso.predict}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{predict}}}}}(model, estimator, task, score, …)
&
A function used to release the score of a model.
\\
\hline
\end{longtable}\sphinxatlongtableend\end{savenotes}


\chapter{Module Summary}
\label{\detokenize{index:module-summary}}\index{pspso (class in pspso)@\spxentry{pspso}\spxextra{class in pspso}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{pspso.}}\sphinxbfcode{\sphinxupquote{pspso}}}{\emph{estimator=\textquotesingle{}xgboost\textquotesingle{}}, \emph{params=None}, \emph{task=\textquotesingle{}regression\textquotesingle{}}, \emph{score=\textquotesingle{}rmse\textquotesingle{}}}{}
This class searches for algorithm parameters by using the Particle Swarm Optimization (PSO) algorithm.
\index{calculatecombinations() (pspso.pspso method)@\spxentry{calculatecombinations()}\spxextra{pspso.pspso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.calculatecombinations}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{calculatecombinations}}}{}{}
A function that will generate all the possible combinations in the search space.
Used mainly with grid search

Returns
\begin{description}
\item[{combinations: list}] \leavevmode
A list that contains all the possible combinations.

\end{description}

\end{fulllineitems}

\index{decode\_parameters() (pspso.pspso static method)@\spxentry{decode\_parameters()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.decode_parameters}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{decode\_parameters}}}{\emph{particle}}{}
Decodes the parameters of a list into a meaningful set of parameters.
To decode a particle, we need the following global variables:parameters,
defaultparameters, paramdetails, and rounding.

\end{fulllineitems}

\index{f() (pspso.pspso static method)@\spxentry{f()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.f}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{f}}}{\emph{q}, \emph{estimator}, \emph{task}, \emph{score}, \emph{X\_train}, \emph{Y\_train}, \emph{X\_val}, \emph{Y\_val}}{}
Higher\sphinxhyphen{}level method to do forward\_prop in the
whole swarm.

Inputs
\begin{description}
\item[{x: numpy.ndarray of shape (n\_particles, dimensions)}] \leavevmode
The swarm that will perform the search

\end{description}

Returns
\begin{description}
\item[{numpy.ndarray of shape (n\_particles, )}] \leavevmode
The computed loss for each particle

\end{description}

\end{fulllineitems}

\index{fitpsgrid() (pspso.pspso method)@\spxentry{fitpsgrid()}\spxextra{pspso.pspso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.fitpsgrid}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fitpsgrid}}}{\emph{X\_train=None}, \emph{Y\_train=None}, \emph{X\_val=None}, \emph{Y\_val=None}}{}
Select the algorithm parameters based on Grid search.

Grid search was implemented to match the training process with pspso and for comparison purposes.
I have to traverse each value between x\_min, x\_max. Create a list seperating rounding value.

\end{fulllineitems}

\index{fitpspso() (pspso.pspso method)@\spxentry{fitpspso()}\spxextra{pspso.pspso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.fitpspso}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fitpspso}}}{\emph{X\_train=None}, \emph{Y\_train=None}, \emph{X\_val=None}, \emph{Y\_val=None}, \emph{psotype=\textquotesingle{}global\textquotesingle{}}, \emph{number\_of\_particles=5}, \emph{number\_of\_iterations=10}, \emph{options=\{\textquotesingle{}c1\textquotesingle{}: 1.49618}, \emph{\textquotesingle{}c2\textquotesingle{}: 1.49618}, \emph{\textquotesingle{}w\textquotesingle{}: 0.7298\}}}{}
Select the algorithm parameters based on PSO.

Inputs
\begin{description}
\item[{X\_train: numpy.ndarray of shape (a,b)}] \leavevmode
Contains the training input features, a is the number of samples, b is the number of features

\item[{Y\_train: numpy.ndarray of shape (a,1)}] \leavevmode
Contains the training target, a is the number of samples

\item[{X\_train: numpy.ndarray of shape (c,b)}] \leavevmode
Contains the validation input features, c is the number of samples, b is the number of features

\item[{Y\_train: numpy.ndarray of shape (c,1)}] \leavevmode
Contains the training target, c is the number of samples

\item[{number\_of\_particles: integer}] \leavevmode
number of particles in the PSO search space.

\item[{number\_of\_iterations: integer}] \leavevmode
number of iterations.

\item[{options: dictionary}] \leavevmode
A key,value dict of PSO parameters c1,c2, and w

\end{description}

Returns
\begin{description}
\item[{pos: list}] \leavevmode
The encoded parameters of the best solution

\item[{cost: float}] \leavevmode
The score of the best solution

\item[{duration: float}] \leavevmode
The time taken to conduct random search.

\item[{model:}] \leavevmode
The best model generated via random search

\item[{combinations: list of lists}] \leavevmode
The combinations examined during random search

\item[{results: list}] \leavevmode
The score of each combination in combinations list

\end{description}

\end{fulllineitems}

\index{fitpsrandom() (pspso.pspso method)@\spxentry{fitpsrandom()}\spxextra{pspso.pspso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.fitpsrandom}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fitpsrandom}}}{\emph{X\_train=None}, \emph{Y\_train=None}, \emph{X\_val=None}, \emph{Y\_val=None}, \emph{number\_of\_attempts=20}}{}
Select the algorithm parameters based on radnom search.

With Random search, the process is done for number of times specified by a parameter in the function.

Inputs
\begin{description}
\item[{X\_train: numpy.ndarray of shape (a,b)}] \leavevmode
Contains the training input features, a is the number of samples, b is the number of features

\item[{Y\_train: numpy.ndarray of shape (a,1)}] \leavevmode
Contains the training target, a is the number of samples

\item[{X\_train: numpy.ndarray of shape (c,b)}] \leavevmode
Contains the validation input features, c is the number of samples, b is the number of features

\item[{Y\_train: numpy.ndarray of shape (c,1)}] \leavevmode
Contains the training target, c is the number of samples

\item[{number\_of\_attempts: integer}] \leavevmode
The number of times random search to be tried.

\end{description}

Returns
\begin{description}
\item[{pos: list}] \leavevmode
The encoded parameters of the best solution

\item[{cost: float}] \leavevmode
The score of the best solution

\item[{duration: float}] \leavevmode
The time taken to conduct random search.

\item[{model:}] \leavevmode
The best model generated via random search

\item[{combinations: list of lists}] \leavevmode
The combinations examined during random search

\item[{results: list}] \leavevmode
The score of each combination in combinations list

\end{description}

\end{fulllineitems}

\index{forward\_prop\_gbdt() (pspso.pspso static method)@\spxentry{forward\_prop\_gbdt()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.forward_prop_gbdt}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{forward\_prop\_gbdt}}}{\emph{particle}, \emph{task}, \emph{score}, \emph{X\_train}, \emph{Y\_train}, \emph{X\_val}, \emph{Y\_val}}{}
Train the GBDT after decoding the parameters in variable particle.
The particle is decoded into parameters of the gbdt. Then, The gbdt is trained and the score is sent back to the fitness function.

Inputs
\begin{description}
\item[{particle: list of values (n dimensions)}] \leavevmode
A particle in the swarm

\item[{task: regression, binary classification}] \leavevmode
the task to be conducted

\item[{score: rmse (regression), auc (binary classification), acc (binary classification)}] \leavevmode
the type of evaluation

\item[{X\_train: numpy.ndarray of shape (m, n)}] \leavevmode
Training dataset

\item[{Y\_train: numpy.ndarray of shape (m,1)}] \leavevmode
Training target

\item[{X\_val: numpy.ndarray of shape (x, y)}] \leavevmode
Validation dataset

\item[{Y\_val: numpy.ndarray of shape (x,1)}] \leavevmode
Validation target

\end{description}

Returns
\begin{description}
\item[{variable, model}] \leavevmode
the score of the trained algorithm over the validation dataset, trained model

\end{description}

\end{fulllineitems}

\index{forward\_prop\_mlp() (pspso.pspso static method)@\spxentry{forward\_prop\_mlp()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.forward_prop_mlp}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{forward\_prop\_mlp}}}{\emph{particle}, \emph{task}, \emph{score}, \emph{X\_train}, \emph{Y\_train}, \emph{X\_val}, \emph{Y\_val}}{}
Train the MLP after the decoding the parameters in variable particle.

\end{fulllineitems}

\index{forward\_prop\_svm() (pspso.pspso static method)@\spxentry{forward\_prop\_svm()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.forward_prop_svm}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{forward\_prop\_svm}}}{\emph{particle}, \emph{task}, \emph{score}, \emph{X\_train}, \emph{Y\_train}, \emph{X\_val}, \emph{Y\_val}}{}
Train the SVM after decoding the parameters in variable particle.

\end{fulllineitems}

\index{forward\_prop\_xgboost() (pspso.pspso static method)@\spxentry{forward\_prop\_xgboost()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.forward_prop_xgboost}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{forward\_prop\_xgboost}}}{\emph{particle}, \emph{task}, \emph{score}, \emph{X\_train}, \emph{Y\_train}, \emph{X\_val}, \emph{Y\_val}}{}
Train the XGBoost after decoding the parameters in variable particle.
The particle is decoded into parameters of the XGBoost.
This function is similar to forward\_prop\_gbdt
The gbdt is trained and the score is sent back to the fitness function.

Inputs
\begin{description}
\item[{particle: list of values (n dimensions)}] \leavevmode
A particle in the swarm

\item[{task: regression, binary classification}] \leavevmode
the task to be conducted

\item[{score: rmse (regression), auc (binary classification), acc (binary classification)}] \leavevmode
the type of evaluation

\item[{X\_train: numpy.ndarray of shape (m, n)}] \leavevmode
Training dataset

\item[{Y\_train: numpy.ndarray of shape (m,1)}] \leavevmode
Training target

\item[{X\_val: numpy.ndarray of shape (x, y)}] \leavevmode
Validation dataset

\item[{Y\_val: numpy.ndarray of shape (x,1)}] \leavevmode
Validation target

\end{description}

Returns
\begin{description}
\item[{variable, model}] \leavevmode
the score of the trained algorithm over the validation dataset, trained model

\end{description}

\end{fulllineitems}

\index{get\_default\_params() (pspso.pspso static method)@\spxentry{get\_default\_params()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.get_default_params}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{get\_default\_params}}}{\emph{estimator}, \emph{task}}{}
Set the default parameters of the estimator.
This function assigns the default parameters for the user.
Each algorithm has a set of parameters. To allow the user to search for some parameters
instead of the supported parameters, this function is used to assign a default value for each parameter.
In addition, it gets other parameters for each algorithm. For e.g, it returns the number of epochs, batch\_size, and loss for the mlp.

Inputs
\begin{description}
\item[{estimator: string value}] \leavevmode
A string value that determines the estimator: ‘mlp’,’xgboost’,’svm’, or ‘gbdt’

\item[{task: string value}] \leavevmode
A string value that determines the task under consideration: ‘regression’ or ‘binary classification’

\end{description}

Returns
\begin{description}
\item[{defaultparams: Dictionary}] \leavevmode
A dictionary that contains default parameters to be used.

\end{description}

\end{fulllineitems}

\index{get\_default\_search\_space() (pspso.pspso static method)@\spxentry{get\_default\_search\_space()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.get_default_search_space}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{get\_default\_search\_space}}}{\emph{estimator}, \emph{task}}{}
Create a dictionary of default parameters if the user didnt provide parameters.

Inputs
\begin{description}
\item[{estimator: string value}] \leavevmode
A string value that determines the estimator: ‘mlp’,’xgboost’,’svm’, or ‘gbdt’

\item[{task: string value}] \leavevmode
A string value that determines the task under consideration: ‘regression’ or ‘binary classification’

\end{description}

Returns
\begin{description}
\item[{params: Dictionary}] \leavevmode
A dictionary that contains default parameters to be used.

\end{description}

\end{fulllineitems}

\index{predict() (pspso.pspso static method)@\spxentry{predict()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{predict}}}{\emph{model}, \emph{estimator}, \emph{task}, \emph{score}, \emph{X\_val}, \emph{Y\_val}}{}
A function used to release the score of a model.
If the score is rmse, the value is released.
If the score is acc (accuracy), 1\sphinxhyphen{}acc is returned back since pso applies a minimization task.
If the score is auc, 1\sphinxhyphen{}auc is returned back since pso applies a minization task

This class is static and can be used to test the model accuracy over the hold\sphinxhyphen{}out sample once the selection process is finalized.

Inputs
\begin{description}
\item[{model:}] \leavevmode
A trained model

\item[{estimator: string value}] \leavevmode
A string value that determines the estimator: ‘mlp’,’xgboost’,’svm’, or ‘gbdt’

\item[{task: string value}] \leavevmode
A string value that determines the task under consideration: ‘regression’ or ‘binary classification’

\item[{score: string value}] \leavevmode
Determines the score (‘rmse’,’auc’,’acc’)

\item[{X\_val: numpy.ndarray}] \leavevmode
Input features

\item[{Y\_val: numpy.ndarray}] \leavevmode
Target

\end{description}

Returns
\begin{description}
\item[{met: float}] \leavevmode
Score value of the model

\end{description}

\end{fulllineitems}

\index{print\_results() (pspso.pspso method)@\spxentry{print\_results()}\spxextra{pspso.pspso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.print_results}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_results}}}{}{}
Print the results found in the pspso instance. Expected to print general details
like estimator, task, selection type, number of attempts examined, total number of
combinations, position of the best solution, score of the best solution, parameters,
details about the pso algorithm.

\end{fulllineitems}

\index{read\_parameters() (pspso.pspso static method)@\spxentry{read\_parameters()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.read_parameters}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{read\_parameters}}}{\emph{params=None}, \emph{estimator=None}, \emph{task=None}}{}
Read the parameters provided by the user.

Inputs
\begin{description}
\item[{params: dictionary of key,values added by the user}] \leavevmode
This dictionary determines the parameters and ranges of parameters the user wants to selection values from.

\item[{estimator: string value}] \leavevmode
A string value that determines the estimator: ‘mlp’,’xgboost’,’svm’, or ‘gbdt’

\item[{task: string value}] \leavevmode
A string value that determines the task under consideration: ‘regression’ or ‘binary classification’

\end{description}

Returns
\begin{description}
\item[{parameters}] \leavevmode
The parameters selected by the user

\item[{defaultparams}] \leavevmode
Default parameters

\item[{x\_min: list}] \leavevmode
The lower bounds of the parameters search space

\item[{x\_max: list}] \leavevmode
The upper bounds of the parameters search space

\item[{rounding: list}] \leavevmode
The rounding value in each dimension of the search space

\item[{bounds: dict}] \leavevmode
A dictionary of the lower and upper bounds

\item[{dimensions: integer}] \leavevmode
Dimensions of the search space

\item[{params: Dict}] \leavevmode
Dict given by the author

\end{description}

\end{fulllineitems}

\index{rebuildmodel() (pspso.pspso static method)@\spxentry{rebuildmodel()}\spxextra{pspso.pspso static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:pspso.pspso.rebuildmodel}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{rebuildmodel}}}{\emph{estimator}, \emph{pos}, \emph{task}, \emph{score}, \emph{X\_train}, \emph{Y\_train}, \emph{X\_val}, \emph{Y\_val}}{}
Used to rebuild the model after selecting the parameters.

\end{fulllineitems}


\end{fulllineitems}



\chapter{Future Work}
\label{\detokenize{index:future-work}}

\section{New Algorithms}
\label{\detokenize{index:new-algorithms}}
Other machine learning algorithms and packages will be added such as the catboost.


\section{Cross Validation}
\label{\detokenize{index:cross-validation}}
We are working towards adding the cross validation support that will take the training data and number of folds.

Then split the records and train each fold. The average performance of cross\sphinxhyphen{}validation will be retuned back to the user.


\section{Multi\sphinxhyphen{}Class Classification}
\label{\detokenize{index:multi-class-classification}}
We are also working on adding multi\sphinxhyphen{}class classification and data oversampling techniques.


\chapter{Contributing}
\label{\detokenize{index:contributing}}
Pull requests are welcome. For major changes, please open an issue first
to discuss what you would like to change.

Please make sure to update tests as appropriate.

We are working towards adding the cross validation support that will take the training data and number of folds, then split the records and train each fold. Finally, the average performance is retuned to the user.

We are also working on adding multi\sphinxhyphen{}class classification and data oversampling techniques.


\chapter{License}
\label{\detokenize{index:license}}
Copyright (c) {[}2020{]} {[}Ali Haidar{]}

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the “Software”), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



\renewcommand{\indexname}{Index}
\printindex
\end{document}